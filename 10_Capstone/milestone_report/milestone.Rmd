---
title: "Data Science Capstone: Milestone Report"
author: "Sean Angiolillo"
date: "6 February 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is my milestone report in support of the JHU Data Science Specialization Capstone on Coursera. My goal is to make a preliminary exploration of the data and lay out a plan to construct a prediction algorithm in the eventual Shiny app.

## Load Libraries and Data

```{r message=FALSE, echo=FALSE}
library(tidyverse)
library(stringr)
library(stringi)
library(tidytext)
library(wordcloud)
library(quanteda)
```

First I'll download the data and calculate the size of each file. I'm choosing only to work with the English language texts.

```{r echo=FALSE}
# download and unzip the files
if (!file.exists("Coursera-SwiftKey.zip")) {
    url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    download.file(url, dest = "Coursera-SwiftKey.zip", mode = "wb") 
    unzip("Coursera-SwiftKey.zip", exdir = "./")
}

# list of files
file_dir <- c("./final/en_US/en_US.blogs.txt",
              "./final/en_US/en_US.news.txt",
              "./final/en_US/en_US.twitter.txt")

# find file sizes in MB
file_size_MB <- vector()
for (file in file_dir) {
    file_size_MB[[file]] <- file.info(file)$size / 1024^2
}
cbind(file_size_MB)
```

We can see these are large files. I tried data.table's `fread` function and stringi's `stri_read_lines` but they had trouble with null strings in the tweets file. The base `readLines` worked fine, but `read_lines` of the tidyverse was faster.

```{r echo=FALSE}
# read files
if (!exists("USblogs")) {
  USblogs <- read_lines("./final/en_US/en_US.blogs.txt")
}
if (!exists("USnews")) {
  USnews <- read_lines("./final/en_US/en_US.news.txt")
}
if (!exists("UStweets")) {
  UStweets <- read_lines("./final/en_US/en_US.twitter.txt")
}
```

Stringi's `stri_stats_general` is a useful function to get the number of lines and characters in each corpus. We can see that we don't have any empty lines.

```{r echo=FALSE}
general_stats <- data.frame(blogs = stri_stats_general(USblogs),
                 news = stri_stats_general(USnews),
                 tweets = stri_stats_general(UStweets),
                 row.names = c("Lines","LinesNEmpty","Chars","CharsNWhite"))
general_stats
```

## Sample Data

Because we are just exploring the data for cleaning and preliminary visualizations, we can take small samples of each file in order to make our computations run faster and view the first two elements of each.

```{r echo=FALSE}
# take a small sample of each file
set.seed(417)
blogs <- sample(USblogs, length(USblogs)*0.001)
rm(USblogs)

set.seed(417)
news <- sample(USnews, length(USnews)*0.001)
rm(USnews)

set.seed(417)
tweets <- sample(UStweets, length(UStweets)*0.001)
rm(UStweets)

# view first 2 of each
writeLines(blogs[1:2])
writeLines(news[1:2])
writeLines(tweets[1:2])
```

In addition to reading a few examples, we can compute a few summary statistics for each file. We have more tweets than blogs or news excerpts, but not surprisingly the tweets are much shorter.

```{r echo=FALSE}
file_list <- list(blogs, news, tweets)
file_sum <- data.frame(
    file = c("blogs","news","tweets"),
    lines = sapply(file_list, length),
    max_word = sapply(file_list, function(x) {max(stri_count_words(x))}), 
    mean_word = sapply(file_list, function(x) {mean(stri_count_words(x))}),
    sum_word = sapply(file_list, function(x) {sum(stri_count_words(x))})
)
file_sum
```


## Choosing Texts

I had to determine which files to use for the project. I was somewhat reluctant to use the tweets because the language it contains would need more cleaning and have more slang and misspellings that would be difficult to correct. I created wordclouds of each to get a sense of the kinds of words each file contained. Because they appear to constitute largely different vocabulary, I'll combine the three samples into one corpus in order to increase language coverage. 

```{r echo=FALSE}
# create a tidy dataframe
blogs_tbl <- as.tibble(blogs)
tidy_blogs <- blogs_tbl %>%
    unnest_tokens(word, value)

# plot a wordcloud
tidy_blogs %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, scale = c(3,0.3)))

# repeat for news
news_tbl <- as.tibble(news)
tidy_news <- news_tbl %>%
    unnest_tokens(word, value)

tidy_news %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, scale = c(3,0.3)))

# repeat for tweets
tweets_tbl <- as.tibble(tweets)
tidy_tweets <- tweets_tbl %>%
    unnest_tokens(word, value)

# remove stopwords and plot wordcloud
tidy_tweets %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, scale = c(3,0.3)))

# merge into one sample
sample <- c(blogs, news, tweets)
rm(blogs)
rm(news)
rm(tweets)
```

## Remove Foreign Characters

One of my first cleaning steps is to remove all foreign characters from the sample. We chose the English files to begin with but we can see that some foreign characters do remain.

```{r echo=FALSE}
# find all unique characters
sample %>% 
  str_extract_all(boundary("character")) %>% 
  unlist() %>% 
  unique()
```

We can remove them with the `iconv` function and confirm their removal. It's possible that some foreign words remain in the corpus, but given that it's supposed to be an English text to begin with, it's fairly unlikely.

```{r echo=FALSE}
# remove foreign characters
sample <- iconv(sample, "latin1", "ASCII", sub = "")

# confirm removal 
sample %>% 
  str_extract_all(boundary("character")) %>% 
  unlist() %>% 
  unique()
```

## Create and Clean a Corpus

After exploring the capabilities of the `tm` package, and reading the mentor advice [here]("https://github.com/lgreski/datasciencectacontent/blob/master/markdown/capstone-simplifiedApproach.md"), I opted to use `quanteda` for corpus cleaning and tokenization.

One of the cleaning tasks specifically given in the instructions is to filter profanity. I found a list of 1300 words from CMU, but this [list]("https://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/") of 550 words seems more reasonable.

```{r echo=FALSE}
# download and unzip profanity list
if (!file.exists("full-list-of-bad-words-banned-by-google-txt-file.zip")) {
    url <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-banned-by-google-txt-file.zip"
    download.file(url, dest = "full-list-of-bad-words-banned-by-google-txt-file.zip", mode = "wb") 
    unzip("full-list-of-bad-words-banned-by-google-txt-file.zip", exdir = "./")
}

# read in file and remove foreign characters
if (!exists("profanity")) {
  profanity <- readLines("full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt", skipNul = TRUE, warn = FALSE)
  profanity <- iconv(profanity, "latin1", "ASCII", sub = "")
}
```

As an exploratory analysis, I wanted to make decisions on the major cleaning questions as opposed to a very fine tooth cleaning. In the final product, I'll likely return to the corpus with more regular expressions.

My cleaning steps however can be summarized as follows:

* convert characters to lower case but keep upper case acronyms

* remove numbers, punctuation, symbols, hyphens, Twitter characters, and URLs

* remove profanity

More steps might be needed in future but this seemed like a good start. I did not perform two common text cleaning operations: removing stopwords and stemming words. While these operations may be useful for tasks like sentiment analysis, it seemed like neither of these were appropriate for predictive word modeling. Stopwords like "a", "an", "the" would seem to be quite important to have in our dataset in order to make good predictions. We can see that we now have clean unigrams stored in a dataframe.

```{r echo=FALSE}
# tokenize sample
tokens <- tokens(char_tolower(sample, keep_acronyms = TRUE), what = "word", 
                     remove_numbers = TRUE, remove_punct = TRUE,
                     remove_symbols = TRUE, remove_hyphens = TRUE,
                     remove_twitter = TRUE, remove_url = TRUE,
                     ngrams = 1L)
rm(sample)
    
# remove profanity
clean_tokens <- tokens_remove(tokens, profanity)
rm(tokens)
    
# unlist tokens and convert to a dataframe
clean_text <- unlist(clean_tokens)
text_df <- data.frame(clean_text, stringsAsFactors = FALSE)
head(text_df$clean_text)
```

## Tidytext Tokenization

I'll be using `quanteda` in the rest of the project to tokenize the text in order to take advantage of Quanteda's use of parallel programming by default, but I also wanted to try out the `tidytext` package. The `unnest_tokens` function makes it very easy to create unigram, bigrams, trigrams, etc. I'll create unigrams, bigrams and trigrams. It would be easy to create higher order n-grams, but Michael Collins' lecture [series]("https://www.youtube.com/playlist?list=PL0ap34RKaADMjqjdSkWolD-W2VSCyRUQC") suggests trigrams are usually tough to beat so I'll stop there.

```{r echo=FALSE}
# create unigrams, bigrams, trigrams
unigrams <- text_df %>%
    unnest_tokens(unigram, clean_text, token = "ngrams", n = 1)

bigrams <- text_df %>%
    unnest_tokens(bigram, clean_text, token = "ngrams", n = 2)

trigrams <- text_df %>%
    unnest_tokens(trigram, clean_text, token = "ngrams", n = 3)

head(unigrams$unigram)
head(bigrams$bigram)
head(trigrams$trigram)
```

## N-gram Visualizations

We can look at the most common instances at each level of tokenization. The plots are key to understanding the distribution of word frequencies. With a larger sample, we'd have a good understanding of the relationships between words and word pairs in the data. Not surprisingly, the most frequent instances of all n-grams are stopwords.

```{r echo=FALSE, message=FALSE}
# plot unigrams
unigrams %>%
    count(unigram, sort = TRUE) %>%
    top_n(15) %>%    
    mutate(unigram = reorder(unigram, n)) %>%
    ggplot(aes(unigram, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip() + 
    ggtitle("Most Frequent Unigrams")

# plot bigrams
bigrams %>%
    count(bigram, sort = TRUE) %>%
    top_n(15) %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(bigram, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip() +
    ggtitle("Most Frequent Bigrams")

# plot trigrams
trigrams %>%
    count(trigram, sort = TRUE) %>%
    top_n(15) %>%
    mutate(trigram = reorder(trigram, n)) %>%
    ggplot(aes(trigram, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip() +
    ggtitle("Most Frequent Trigrams")
```

## Coverage

Our intuition, as well as the plots above, demonstrate the sharp declining frequency of words. Accordingly, in order to increase coverage, we see an exponential relationship. Perhaps more accurately, we could call it a Zipfian distribution because it follows [Zipf's law]("https://en.wikipedia.org/wiki/Zipf%27s_law"), which states that given some corpus of natural language, the frequency of any word is inversely proportional to its inverse rank in the frequency table. Accordingly for our purposes, because some words have such high frequency adding them to the corpus raises coverage by much more than adding less common words.

```{r echo=FALSE}
# calculate coverage
uni_counts <- unigrams %>%
    count(unigram, sort = TRUE) %>%
    rownames_to_column("word_rank") %>%
    mutate(coverage = cumsum(n) / nrow(unigrams))

plot(x = uni_counts$coverage, y = uni_counts$word_rank, 
     type = "l", ylab = "Coverage", xlab = "Number of Unique Words", 
     main = "Unique words versus dictionary coverage")
```

The most frequent 10 words covers approximately 20% of all word instances in the sample. The most frequent 100 words covers approximately 46% of all word instances. To get to 90% coverage, we need 5707 words. While this is only a small sample, the same trend will be present in larger samples.

This suggests that when building the dataset our prediction algorithm will operate upon, we will really need a lot of words to get a very high level of coverage. On the other hand, for a slightly lower level of coverage (say 80%) we would need far less. 

So by setting a fixed level of coverage, eg 95%, or removing any n-gram below a certain threshold count, we can save a lot of memory with only losing a small amount of very low-frequency words. This is called pruning I believe.

Another way to increase coverage would be to stem words, essentially grouping similar words into a single n-gram (which reduces the overall count). However, we've decided against this as it would hinder predictions. Another idea could be to substitute synonyms for less common words using a thesaurus. This sounds like a pretty reasonable idea, but I haven't found much information on how to implement it, and so I'll forgo it.

## Handling Unseen N-grams

At the core of our language model will be the Markov independence assumption that the probability of the next word depends only on the previous k words. While this is an obviously incorrect assumption because it ignores very real long-range dependencies in the structure of language, it drastically simplifies the task at hand by reducing the number of parameters. This assumption allows us to treat the frequencies in our n-gram tables as the training data for our prediction algorithm.

However, as this is an open-ended vocabulary problem, it is very likely that the model will encounter n-grams that it has never before seen in the training data. We could also guess this from our Zipfian distribution. In this case, the naive model would peg the probability of an unseen n-gram as 0. Accordingly, it most certainly will not generalize well to new words in the testing data.

This is a problem we can try to address with a discounting method. Essentially we transfer some small amount of probability (missing probability mass) in our event space away from the maximum likelihood estimate (MLE) of seen n-grams towards unseen n-grams while ensuring we maintain a valid probability distribution. Various "smoothing" algorithms include Laplace Add-1 estimation, Good-Turing smoothing, or Kneser-Ney smoothing.

My plan is to build a simple backoff model instead of using any of these more complicated techniques. If I encounter a previously unseen word, one option could be just to produce a very common word as the prediction.

## Model Evaluation

I understand the need to break the corpus into a training and testing set, but the concept of evaluating this type of model isn't entirely clear yet to me. It seems very unlike a typical classification or a regression problem. On a basic level I understand a better language model assigns higher probability to a common sentence than a poor one, but ultimately this is an area in which I need to do more research. Moreover, it seems with a backoff model I'm trying to build, I won't actually be calculating probabilities. I'll instead rely solely on frequencies so it's not possible to calculate perplexity, a common measurement for language models.

## Next Steps

With some exploration and visualization of the data complete, I can lay out a plan to complete the project.

* I'll likely want to improve cleaning with some regular expressions. It might help to add an end of sentence tag to mark where sentences end and new ones begin.

* I'll write a function to generate separate files of n-grams of varying size for each file (news, blogs, tweets). The `quanteda::tokens_ngrams()` function will be used for tokenization to take advantage of parallel processing.

* I will likely need to sample the files beforehand because I only have 4GB of memory on my machine. 

* I'll combine these n-gram files into the more memory-efficient data tables (instead of dataframes). Splitting off the last word of the n-gram will give the prediction, leaving the base. I'll then calculate the frequency of each n-gram.

* I'll have to consider pruning or removing some low frequency counts in order to increase coverage of other words. As far as I can tell, I can do such a removal not only for low count unigrams, but also for higher order n-grams. For instance, consider the bigram "I want". The most frequent next word might be "to". We might have hundreds of other words following "I want" ("pizza", "jeans", etc.). But if we have low counts of "I want unicycles", we'd never make this prediction because we have so many better alternatives so we can remove it from our table. I think that makes sense? 

* I'll use these tables as the data for a backoff model, using the `sqldf` package in order to query the data tables, hopefully in an efficient way. Based on the frequency of the n-gram tables, I can find the most probable prediction.

* The backoff model will first look for the most frequent prediction among trigrams. If it does not find any, it backs off to look for a prediction amongst bigrams. If it doesn't find a prediction amongst bigrams, I suppose it could give the most common unigram.

* Lastly, I will incorporate this model into a Shiny app.

What has been surprising is that, as far as I can tell, there is no readily available package with functions to implement a backoff model. It seems we have to code our own algorithm. Moreover, there seems to be much less information overall regarding this topic. There are academic papers, professor slides (most of which are shockingly similar), and a few youtube videos from the same professors. Perhaps this capstone has brought this topic into the purview of a more mainstream audience for the first time.