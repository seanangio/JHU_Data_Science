---
title: "Supervised Learning: Classification of Exercise"
author: "Sean Angiolillo"
date: "14 December 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

## Introduction

In this investigation, we will try to classify the manner in which an exercise was done using accelerometer, gyroscope and magnetometer data. In an [experiment](http://groupware.les.inf.puc-rio.br/har), under expert supervision, six participants performed the same exercise (a bicep curl) in the correct fashion and four common mistakes.

These mistakes included throwing elbows to the front, lifting the dumbbell only halfway, lowering the dumbbell only halfway, and throwing the hips to the front. Data was collected from three instruments on the participant's belt, forearm, arm, and dumbbell.

Being able to classify the manner in which an exercise was completed, rather than just the quantity of exercises finished, would have a number of important applications related to exercise safety and performance.

The given task is really focused only on prediction, and so the tradeoffs we associate with achieving greater accuracy (reduced scalability, interpretability, speed or simplicity) can be ignored in favor of better predictions. 

It also does not seem like any kind of mistake is more damaging than another so we won't need to make a certain kind of mistake more costlier than others. That is, we can treat false negatives and false positives equally. Overall accuracy, rather than the sensitivity or specificity, will be our guiding metric.

## Data Preparation
### Data Loading

```{r message=FALSE}
# load libraries
library(caret)
library(plyr)
library(dplyr)
library(reshape2)
library(ggplot2)
library(gridExtra)
library(class)
library(gmodels)
library(naivebayes)
library(C50)
library(rpart)
library(randomForest)
```


First we need to read in the data. Note that I've called the provided testing set the validation set, as I'll have data sampled from the training set that I'll refer to as the testing data.

```{r}
# download and read in training if not present
if (!file.exists("training.csv")) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(url, dest = "training.csv", mode = "wb") 
}

if (!exists("trainingRaw")) {
  trainingRaw <- read.csv("training.csv")
}

# download and read in testing if not present
if (!file.exists("testing.csv")) {
    url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(url, dest = "testing.csv", mode = "wb") 
}

if (!exists("testing")) {
  validation <- read.csv("testing.csv")
}
```

After reading in the data, we have an initial training set of `r dim(trainingRaw)[1]` rows and `r dim(trainingRaw)[2]` columns, one of which is the outcome variable, `classe`, that we want to predict. Our validation set is only `r dim(validation)[1]` observations.

### Handle Missing Data

Missing data is a problem for many of the machine learning algorithms we'd like to run to build our models, and one of the first things to notice when looking at this training set is the prevalence of missing data. Out of `r nrow(trainingRaw)` observations, we have only `r sum(complete.cases(trainingRaw))` complete cases.

```{r}
# instances of missing data in each column in training set
missingCols <- cbind(colSums(is.na(trainingRaw)))

# samples of complete and missing predictors
cbind(missingCols[1:5,])
cbind(missingCols[27:32,])
```

Looking at the missing data column-wise, we can see that we have two types of columns with regard to missing data in the training set: `r colSums(missingCols == 0)` columns fully complete and `r colSums(missingCols != 0)` columns each missing `r round(max(missingCols),0)` observations.

Rather than trying a strategy like median or k-nearest neighbors imputation on the missing data, it is important to realize that these columns actually represent summary statistics for the other columns, for instance, columns beginning with `avg`, `stddev` or `var`. These summary statistics correspond to the binary `new_window` variable, and so we can be confident in removing these columns (as well as `new_window`) from consideration in training and validation sets.

(As a side note, if we were interested in interpretability, it may be interesting to try to build a model only with the summary statistics.) 

Because we are tasked with making our predictions from only the measurements, I'll also remove the first few predictor variables including some of the time series data.

```{r}
# find summary predictors
sum_stats <- c("max","min","amplitude","avg","stddev","kurtosis",
               "skewness","total", "var")
drops <- unique(grep(paste(sum_stats, collapse = "|"),
                     names(trainingRaw), value = TRUE))

# find other unnecessary predictors
non_numeric_cols <- c("X", "user_name", "raw_timestamp_part_1",
           "raw_timestamp_part_2","cvtd_timestamp","new_window",
           "num_window")

# compete list of columns to be dropped
drops <- append(drops, non_numeric_cols)

# drop columns in training and validation sets
training <- trainingRaw[ , !(names(trainingRaw) %in% drops)]
validation <- validation[ , !(names(validation) %in% drops)]
```

### Checking Data Types

Now we have reduced the number of predictor variables from `r ncol(trainingRaw) - 1` to `r ncol(trainingRaw) - 1`. The `str` function shows that some predictors are stored as numeric, while others are integer. I'll make them all numeric.

```{r}
# convert measurements to numeric variables
training[,1:48] <- lapply(training[,1:48], as.numeric)
validation[,1:48] <- lapply(validation[,1:48], as.numeric)
```

### Check for Near Zero Variance Predictors

After removing columns with missing data, we can check for predictor variables with zero or near zero variance because these would likely be poor predictors. It returns 0 rows so we have no concerns from this perspective.

```{r}
# calculate zero and near zero variance predictors
nearzero <- nearZeroVar(training, saveMetrics = TRUE)
nrow(nearzero[nearzero$nzv == TRUE,])
```

### Check for Multicollinearity

We should also check our predictor variables for multicollinearity. We can do this with a correlation matrix.

```{r}
# create correlation matrix
cor.matrix <- cor(training[,-49])

# set correlations of variables with themselves to 0 
diag(cor.matrix) <- 0

# plot correlation matrix
ggplot(melt(cor.matrix), aes(x = Var1, y = Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient2(limits = c(-1, 1)) +
    theme(axis.text.x = element_text(angle=-90, vjust=0.5, hjust=0)) +
    labs(x = "", y = "")
```

We can see the most highly correlated variables in a table.

```{r}
# find most highly correlated predictors
high_cors <- filter(arrange(melt(cor.matrix), -abs(value)), abs(value) > 0.8)
high_cors[-seq(0, nrow(high_cors), 2),]
```

Some variables do appear to be highly correlated. We have `r nrow(high_cors[-seq(0, nrow(high_cors), 2),])` pairs of variables with a correlation above 0.8.

Because our goal is only prediction, as opposed to interpretation, there is not too much to gain by trying to remove further variables from our model. Without a better understanding of the underlying physics, the reasons for these correlations are not immediately apparent.

### Predictor Summary

To summarize our `r ncol(training)-1` predictors, we have three instruments (accelerometer, gyroscope and magnetometer) taking measurements in four places (belt, arm, forearm, and dumbbell) plus the roll, pitch and yaw measurements for each.

```{r}
# predictor variables
names(training[,1:48])
```

I won't plot all of them here, but we get a wide range of distributions. Few if any could be considered approximately normal.

```{r message=FALSE}
# a few distributions of predictor variables
p1 <- ggplot(training, aes(x = roll_belt)) + geom_histogram()
p2 <- ggplot(training, aes(x = magnet_belt_y)) + geom_histogram()
p3 <- ggplot(training, aes(x = gyros_arm_z)) + geom_histogram()
p4 <- ggplot(training, aes(x = accel_forearm_x)) + geom_histogram()
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

Lastly, I want to check for unbalanced outcomes in the data. Having vastly more than one type of class than another will impact how we interpret the accuracy of our results. While class A is more common than the others, it does not appear different enough to affect our models in a dataset of this size.

```{r}
# distribution of classe outcome
round(prop.table(table(training$classe)),4)
```

It is a bit difficult to visualize all of the data. We could make a series of scatterplot matrices like the one below, but it's not unreasonable to expect non-linear models to perform better in this task.

```{r}
# scatterplot matrix for first 8 predictors
featurePlot(x = training[,1:8], y = training$classe, plot = "pairs")
```

### Data Slicing

Having explored the data and selected our feature variables, the next step would be to extract a hold out set from the training data before building models. I've done a 70/30 split here.

```{r}
set.seed(417)
# 70/30 split of training data
intrain <- createDataPartition(y = training$classe, p = 0.7, 
                               list = FALSE)
train <- training[intrain,]
test <- training[-intrain,]

# confirm dimensions
dim(train); dim(test)
```

Because we randomly sampled from a large dataset, the structure of the problem remains the same in the training and test set.

```{r}
# distribution of classe outcome in training, train and test sets
round(prop.table(table(training$classe)),4)
round(prop.table(table(train$classe)),4)
round(prop.table(table(test$classe)),4)
```

## Model Exploration

Having explored and prepared the dataset, I can now experiment with a number of different algorithms to build a successful prediction model.

### K-Nearest Neighbors

I suspect the relationships among our predictors and the classes is complicated but when taken together, I imagine the classes to be fairly homogenous. Accordingly, the first algorithm I'll try to use is k-nearest neighbors because it is a simple and widely used algorithm for classification. Because we have no desire to generate an underlying theory about the data, a non-parametric model like k-nearest neighbors is appropriate.

After standardizing the data, the challenge in applying this algorithm will be in choosing k. I'll start from 1 and compare it to results with larger instances of k.

```{r}
# standardize data
train_z <- as.data.frame(scale(train[-49]))
test_z <- as.data.frame(scale(test[-49]))

set.seed(417)

#define range for k values and initializes accs
range <- 1:10
accs <- rep(0, length(range))

for (k in range) {
  # make predictions using knn
  pred <- knn(train_z, test_z, train[,49], k = k)

  # construct the confusion matrix
  conf <- table(test[,49], pred)

  # calculate and store the accuracy
  accs[k] <- mean(pred == test[,49])
}

# Plot the accuracies
plot(range, accs, xlab = "k")
```

Although we only tried a maximum k of 10, it seems a clear decrease in accuracy as k increases. Below is the confusion matrix for our most accurate result.

```{r}
# confusion matrix when k = 1
pred.knn <- knn(train = train_z, test = test_z,
                      cl = train[,49], 
                      k = 1)

# Create the cross tabulation of predicted vs. actual
CrossTable(x = test[,49], y = pred.knn,
           prop.chisq = FALSE)
```

This amounts to an accuracy of `r round(mean(pred.knn == test[,49]) * 100, 2)`%. This is pretty good, but because we took only one test sample (i.e. we did not do any cross-validation), we may be susceptible to overfitting.

Because cross-validation is easiest to implement with `caret`, I'll build the model there. It can also normalize the data for us. In creating a `train` object, we'll also get a lot of other useful information about the model's performance.

```{r}
# 10 fold cv
ctrl <- trainControl(method = "repeatedcv", number = 10)

set.seed(417)
# fit a knn model with caret
fit.knn.cv <- train(classe ~., data = train, method = "knn",
                 trControl = ctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 5)
# generate predictions
pred.knn.cv <- predict(fit.knn.cv, newdata = test)

# create a confusion matrix
confusionMatrix(pred.knn.cv, test$classe)
```

In terms of accuracy, this model did very well but not as well as the previous attempt without cross-validation. Still, this would likely be a more accurate out of sample error estimate. However, if we examine the model object, we see that caret started with k = 5 and increased from there, and so it didn't try k = 1. I couldn't yet figure out how to specify caret to try that!

### Naive Bayes

Naive Bayes is another popular algorithm for classification tasks that attempts to calculate probabilities for each outcome based on the evidence in the predictors.

```{r}
set.seed(417)
# fit a naive bayes model and generate predictions
fit.nb <- naive_bayes(classe ~ ., data = train)
pred.nb <- predict(fit.nb, newdata = test, type = "class")

# Create the cross tabulation of predicted vs. actual
CrossTable(x = test[,49], y = pred.nb,
           prop.chisq = FALSE)
```

Although still better than random guessing, we see that this model did very poorly. It has an accuracy of only `r round(mean(pred.nb == test[,49]) * 100, 2)`%. I'm sure we could improve results with some better adjustments (and cross-validation), but given that our previous model resulted in 98% accuracy with little work, this seems like the wrong algorithm for this problem.

### Decision Trees (rpart)

Decision trees are another powerful classifier that are often a good option when interpretability is a priority. They use recursive partitioning to divide the dataset into likely outcomes. I'll first use the `rpart` package.

```{r}
set.seed(417)
# Grow a tree
fit.rpart <- rpart(classe ~ ., data = train, method = "class")

# Make predictions on the test dataset
pred.rpart <- predict(fit.rpart, test, type = "class")

# Examine the confusion matrix
table(test$classe, pred.rpart)

# Compute the accuracy on the test dataset
mean(test$classe == pred.rpart)
```

This model performed poorer than our previous efforts. Let's add cross validation by fitting the model in caret. 

```{r}
# Run algorithms using 5-fold cross validation
ctrl <- trainControl(method="cv", number = 10)

# fit the model
fit.rpart.caret <- train(classe ~ ., data = train, method = "rpart",
                         trControl = ctrl)

# Make predictions on the test dataset
pred.rpart.caret <- predict(fit.rpart.caret, test)

# Examine the confusion matrix
table(test$classe, pred.rpart.caret)

# Compute the accuracy on the test dataset
mean(test$classe == pred.rpart.caret)
```

### Decision Trees (C5.0)

Next, I'll try to build a decision tree using the C5.0 algorithm, which is another version of a classification tree.

```{r}
set.seed(417)
fit.C5 <- C5.0(classe ~ ., data = train, trials = 1, costs = NULL)

# Make predictions on the test dataset
pred.C5 <- predict(fit.C5, test, type = "class")

# Examine the confusion matrix
table(test$classe, pred.C5)

# Compute the accuracy on the test dataset
mean(test$classe == pred.C5)
```

I anticipated the C5.0 algorithm to be very similar to our `rpart` result, but this does much better.

To get this level of accuracy, our tree went `r fit.C5$size` decisions deep. In this case, we are not too concerned with interpretability, but we would want to do considerable pruning if that were the case. As we've done before, I'll fit the model in caret using cross validation.

```{r}
# Run algorithms using 10-fold cross validation
ctrl <- trainControl(method="cv", number = 10)

set.seed(417)
# fit the model
fit.C50.caret <- train(classe ~ ., data = train, method = "C5.0",
                         trControl = ctrl)

# Make predictions on the test dataset
pred.C50.caret <- predict(fit.C50.caret, test)

# Examine the confusion matrix
table(test$classe, pred.C50.caret)

# Compute the accuracy on the test dataset
mean(test$classe == pred.C50.caret)
```

Our accuracy has increased even higher.

### Boosted Decision Tree

A simple modification of the C.50 decision tree model is to do repeated trials, which can be considered a form of boosting. While it will take longer to run, it should produce more accurate results. Particularly since we didn't use cross validation here, running 10 trials is a good idea to get a better estimate of out of sample error.

```{r}
set.seed(417)
# fit a boosted decision tree
fit.C5b <- C5.0(classe ~ ., data = train, trials = 10)

# Make predictions on the test dataset
pred.C5b <- predict(fit.C5b, test, type = "class")

# Examine the confusion matrix
table(test$classe, pred.C5b)

# Compute the accuracy on the test dataset
mean(test$classe == pred.C5b)
```

Across these 10 iterations, our average tree size shrunk to `r mean(fit.C5b$size)`, and the accuracy was very high.

### Random Forest

Now that we have done a few decision tree models, we can try a random forest, where the idea is to build a number of trees (a forest) and use a vote to combine the predictions from the trees. This would be a form of ensembling, and often yields very accurate predictions while being robust to overfitting.

```{r}
set.seed(417)
# Grow a tree
fit.rf <- randomForest(classe ~ ., data = train, method = "class")

# Make predictions on the test dataset
pred.rf <- predict(fit.rf, test, type = "class")

# Examine the confusion matrix
table(pred.rf, test$classe)

# Compute the accuracy on the test dataset
mean(pred.rf == test$classe)
```

Alternatively we could build this in caret, where we can more easily include cross-validation.

```{r}
ctrl <- trainControl(method = "cv", 10)
set.seed(417)
# fit the model
fit.rf.caret <- train(classe ~ ., data = train, method = "rf",
                trControl = ctrl, ntree = 250)

# Make predictions on the test dataset
pred.rf.caret <- predict(fit.rf.caret, test)

# Examine the confusion matrix
confusionMatrix(pred.rf.caret, test$classe)
```

## Random Forest (Ranger)

Just for demonstration we could specify `ranger` instead of `rf` as it is designed to be a faster implementation with similar results.

```{r message=FALSE}
ctrl <- trainControl(method = "cv", 10, verboseIter = FALSE)
set.seed(417)
# Fit random forest with ranger
fit.ranger <- train(classe ~ ., data = train, 
                    method = "ranger", 
                    trControl = ctrl)

pred.ranger <- predict(fit.ranger, test)

confusionMatrix(pred.ranger, test$classe)
```

### Stacked Model

The winner of the Netflix prize was one indication that ensembling models can lead to very good predictions. And since scalability is not a concern to us, let's try to stack some of our previous models together and vote on their combined predictions.

```{r}
# combine predictions of the best models into one df
pred.df <- data.frame(pred.knn.cv, 
                     pred.C5, pred.rf, classe = test$classe)

set.seed(417)
# train a model using combined df
fit.comb <- train(classe ~ . , method = "ranger", data = pred.df)

# generate predictions
pred.comb <- predict(fit.comb, pred.df)

# create a confusion matrix
confusionMatrix(pred.comb, test$classe)
```

Although it wasn't entirely necessary given our previous levels of accuracy, this approach also yielded very accurate predictions.

## Predictions on Final Validation Set

We have a number of models to potentially choose from in order to apply to the validation set. After successfully completing the exercise with the random forest model, I saved the correct answers into a vector in order to test some of our best models and see the results. Our random forest models and C5.0 decision trees correctly predicted all 20 observations in the validation set.

```{r}
# correct answers from the quiz
answers <- data.frame(question = 1:20,
                      pred = c("B","A","B","A","A","E","D","B","A", 
                               "A","B","C", "B","A","E","E","A", 
                               "B","B","B"))

# compare predictions on validation set with correct answers
identical(predict(fit.C5b, validation, type = "class"), answers$pred)
identical(predict(fit.C5, validation, type = "class"), answers$pred)
identical(predict(fit.C50.caret, validation), answers$pred)
identical(predict(fit.ranger, validation), answers$pred)
identical(predict(fit.rf.caret, validation), answers$pred)
```
